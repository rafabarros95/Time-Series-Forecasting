{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "def split_train_test(df, test_size):\n",
    "    df = df.sort_values('ds').reset_index(drop=True)\n",
    "    train_size = int(len(df) * (1 - test_size))\n",
    "    return df.iloc[:train_size], df.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_comprehensive_cv_results(df, forecast_func, n_splits=5, title=\"Time Series Cross-Validation Results\", \n",
    "                                   metric='mape', save_to_json='cv_results.json', method_name=None, dataset_name=None):\n",
    "    \"\"\"\n",
    "    Create a comprehensive plot showing the entire time series with all CV splits,\n",
    "    forecasted vs actual values, and training/test periods clearly marked.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with 'ds' and 'y' columns\n",
    "    forecast_func : function that takes (train_df, test_df) and returns predictions array\n",
    "    n_splits : number of CV folds\n",
    "    title : title for the plot\n",
    "    metric : str, either 'mape' or 'rmse' (default: 'mape')\n",
    "    save_to_json : str, path to JSON file to save results (optional)\n",
    "    method_name : str, name of the forecasting method (auto-inferred from function if not provided)\n",
    "    dataset_name : str, name of the dataset (auto-inferred if not provided)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with 'scores' (list of metric scores for each fold), 'mean_score', and 'fold_results'\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    # Validate metric parameter\n",
    "    if metric not in ['mape', 'rmse']:\n",
    "        raise ValueError(\"metric must be either 'mape' or 'rmse'\")\n",
    "    \n",
    "    # Auto-infer method name from function if not provided\n",
    "    if save_to_json is not None and method_name is None:\n",
    "        method_name = getattr(forecast_func, '__name__', 'unknown_method')\n",
    "        print(f\"Auto-inferred method name: '{method_name}'\")\n",
    "    \n",
    "    # Auto-infer dataset name if not provided\n",
    "    if save_to_json is not None and dataset_name is None:\n",
    "        # Try to infer dataset name from common patterns\n",
    "        dataset_name = df.name\n",
    "        print(f\"Auto-inferred dataset name: '{dataset_name}'\")\n",
    "    \n",
    "    # Use the same logic as the original time_series_cv function\n",
    "    df = df.sort_values('ds').reset_index(drop=True)\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=round(len(df) * 0.1))\n",
    "    \n",
    "    scores = []\n",
    "    fold_results = []\n",
    "    \n",
    "    # Get metric name for display\n",
    "    metric_name = metric.upper()\n",
    "    metric_unit = '%' if metric == 'mape' else ''\n",
    "    \n",
    "    # Run the cross-validation (same as original function)\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
    "        train_df = df.iloc[train_idx].copy()\n",
    "        test_df = df.iloc[test_idx].copy()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            predictions = forecast_func(train_df, test_df)\n",
    "            \n",
    "            # Convert predictions to numpy array and ensure correct length\n",
    "            predictions = np.array(predictions)\n",
    "            actual = test_df['y'].values\n",
    "            \n",
    "            if len(predictions) != len(actual):\n",
    "                predictions = predictions[:len(actual)]\n",
    "            \n",
    "            \n",
    "            # Calculate metric score\n",
    "            if metric == 'mape':\n",
    "                mask = actual != 0\n",
    "                if mask.any():\n",
    "                    # Use numpy arrays for both actual and predictions\n",
    "                    actual_masked = actual[mask]\n",
    "                    predictions_masked = predictions[mask]\n",
    "                    fold_score = np.mean(np.abs((actual_masked - predictions_masked) / actual_masked)) * 100\n",
    "                else:\n",
    "                    fold_score = np.inf\n",
    "            elif metric == 'rmse':\n",
    "                fold_score = np.sqrt(np.mean((actual - predictions) ** 2))\n",
    "            \n",
    "            scores.append(fold_score)\n",
    "            fold_results.append({\n",
    "                'fold': fold+1,\n",
    "                'train_dates': (train_df['ds'].min(), train_df['ds'].max()),\n",
    "                'test_dates': (test_df['ds'].min(), test_df['ds'].max()),\n",
    "                'predictions': predictions,\n",
    "                'actual': actual,\n",
    "                'test_df': test_df\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in fold {fold+1}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            scores.append(np.inf)\n",
    "    \n",
    "    # Calculate mean score (same as original)\n",
    "    valid_scores = [s for s in scores if np.isfinite(s)]\n",
    "    mean_score = np.mean(valid_scores) if valid_scores else np.inf\n",
    "    std_score = np.std(valid_scores) if valid_scores else 0\n",
    "    \n",
    "    # Create the visualization - SINGLE PLOT ONLY\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "    \n",
    "    # Sort data by date\n",
    "    df_sorted = df.sort_values('ds').reset_index(drop=True)\n",
    "    \n",
    "    # Plot the entire time series as background (lighter)\n",
    "    ax.plot(df_sorted['ds'], df_sorted['y'], \n",
    "            color='lightgray', alpha=0.5, linewidth=1, label='Complete Time Series')\n",
    "    \n",
    "    # Color palette for different folds\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_splits))\n",
    "    \n",
    "    # Plot each fold's results\n",
    "    for i, fold_data in enumerate(fold_results):\n",
    "        if 'predictions' not in fold_data:\n",
    "            continue\n",
    "            \n",
    "        fold_num = fold_data['fold']\n",
    "        test_start, test_end = fold_data['test_dates']\n",
    "        test_df = fold_data['test_df']\n",
    "        \n",
    "        # Plot actual values for this test period (thick black line)\n",
    "        ax.plot(test_df['ds'], test_df['y'], \n",
    "                color='black', linewidth=4, alpha=0.9,\n",
    "                label='Actual Values (Test Periods)' if i == 0 else \"\")\n",
    "        \n",
    "        # Plot predictions for this test period\n",
    "        predictions = fold_data['predictions'][:len(test_df)]\n",
    "        \n",
    "        \n",
    "        ax.plot(test_df['ds'], predictions, \n",
    "                color=colors[i], linewidth=3, linestyle='--',\n",
    "                marker='o', markersize=6, alpha=0.9,\n",
    "                label=f'Fold {fold_num} Forecast')\n",
    "        \n",
    "        # Add shaded region for test period\n",
    "        ax.axvspan(test_start, test_end, alpha=0.15, color=colors[i])\n",
    "        \n",
    "        # Add performance annotation\n",
    "        fold_score = scores[i] if i < len(scores) and np.isfinite(scores[i]) else np.nan\n",
    "        if not np.isnan(fold_score):\n",
    "            mid_date = test_start + (test_end - test_start) / 2\n",
    "            y_range = ax.get_ylim()[1] - ax.get_ylim()[0]\n",
    "            y_pos = ax.get_ylim()[0] + y_range * 0.05\n",
    "            score_text = f'{metric_name}: {fold_score:.1f}{metric_unit}'\n",
    "            ax.text(mid_date, y_pos, score_text, \n",
    "                    ha='center', va='bottom', fontsize=10, weight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.9, edgecolor=colors[i]))\n",
    "        \n",
    "        # Add fold boundary line\n",
    "        ax.axvline(test_start, color=colors[i], alpha=0.7, linestyle=':', linewidth=2)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Value', fontsize=12)\n",
    "    title_text = f'{title}\\nMean {metric_name}: {mean_score:.2f}{metric_unit} Â± {std_score:.2f}{metric_unit}'\n",
    "    ax.set_title(title_text, fontsize=14, pad=20)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "       \n",
    "    # Prepare return data\n",
    "    results = {\n",
    "        'scores': scores,\n",
    "        'mean_score': mean_score,\n",
    "        'std_score': std_score,\n",
    "        'fold_results': fold_results\n",
    "    }\n",
    "    \n",
    "    # Save to JSON file if requested\n",
    "    if save_to_json is not None:\n",
    "        try:\n",
    "            # Read existing JSON file or create empty dict\n",
    "            if os.path.exists(save_to_json):\n",
    "                with open(save_to_json, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "            else:\n",
    "                existing_data = {}\n",
    "            \n",
    "            # Convert fold_results to JSON-serializable format\n",
    "            json_fold_results = []\n",
    "            for fold_data in fold_results:\n",
    "                json_fold_data = {\n",
    "                    'fold': fold_data['fold'],\n",
    "                    'train_dates': [fold_data['train_dates'][0].isoformat(), fold_data['train_dates'][1].isoformat()],\n",
    "                    'test_dates': [fold_data['test_dates'][0].isoformat(), fold_data['test_dates'][1].isoformat()],\n",
    "                    'predictions': fold_data['predictions'].tolist() if hasattr(fold_data['predictions'], 'tolist') else list(fold_data['predictions']),\n",
    "                    'actual': fold_data['actual'].tolist() if hasattr(fold_data['actual'], 'tolist') else list(fold_data['actual']),\n",
    "                    'test_df_dates': fold_data['test_df']['ds'].dt.strftime('%Y-%m-%d').tolist(),\n",
    "                    'test_df_values': fold_data['test_df']['y'].tolist()\n",
    "                }\n",
    "                json_fold_results.append(json_fold_data)\n",
    "            \n",
    "            # Create the dataset results structure\n",
    "            dataset_results = {\n",
    "                'dataset_name': dataset_name,\n",
    "                'metric': metric,\n",
    "                'n_splits': n_splits,\n",
    "                'mean_score': float(mean_score) if np.isfinite(mean_score) else None,\n",
    "                'std_score': float(std_score) if np.isfinite(std_score) else None,\n",
    "                'scores': [float(s) if np.isfinite(s) else None for s in scores],\n",
    "                'fold_results': json_fold_results\n",
    "            }\n",
    "            \n",
    "            # Update the existing data structure: method_name -> list of dataset results\n",
    "            if method_name not in existing_data:\n",
    "                existing_data[method_name] = []\n",
    "            \n",
    "            # Check if this dataset already exists for this method and update/append\n",
    "            dataset_updated = False\n",
    "            for i, existing_dataset in enumerate(existing_data[method_name]):\n",
    "                if existing_dataset['dataset_name'] == dataset_name:\n",
    "                    existing_data[method_name][i] = dataset_results  # Update existing\n",
    "                    dataset_updated = True\n",
    "                    break\n",
    "            \n",
    "            if not dataset_updated:\n",
    "                existing_data[method_name].append(dataset_results)  # Add new dataset\n",
    "            \n",
    "            # Write back to file\n",
    "            with open(save_to_json, 'w') as f:\n",
    "                json.dump(existing_data, f, indent=2)\n",
    "            \n",
    "            print(f\"\\nResults saved to {save_to_json} under method '{method_name}' for dataset '{dataset_name}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Failed to save results to JSON file: {e}\")\n",
    "    \n",
    "    # Return same structure as original function\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timefm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
